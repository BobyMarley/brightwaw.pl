#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ê–ì–†–ï–°–°–ò–í–ù–´–ô –ê–ù–ê–õ–ò–ó –î–£–ë–õ–ò–ö–ê–¢–û–í –ò –ò–ó–ë–´–¢–û–ß–ù–û–°–¢–ò –í CSS
–ù–∞—Ö–æ–¥–∏—Ç: –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –ø—Ä–∞–≤–∏–ª–∞, –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
"""

import re
from collections import defaultdict
import hashlib

CSS_FILE = 'index.css'

def parse_css_rules(content):
    """–ü–∞—Ä—Å–∏—Ç CSS –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ –ø—Ä–∞–≤–∏–ª–∞"""
    # –£–¥–∞–ª—è–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
    content = re.sub(r'/\*.*?\*/', '', content, flags=re.DOTALL)
    
    rules = []
    media_queries = []
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ–¥–∏–∞-–∑–∞–ø—Ä–æ—Å—ã
    media_pattern = r'@media\s*([^{]+)\s*{((?:[^{}]|{[^{}]*})*)}' 
    for match in re.finditer(media_pattern, content, re.DOTALL):
        media_query = match.group(1).strip()
        media_content = match.group(2)
        media_queries.append({
            'query': media_query,
            'content': media_content,
            'full': match.group(0)
        })
    
    # –£–¥–∞–ª—è–µ–º –º–µ–¥–∏–∞-–∑–∞–ø—Ä–æ—Å—ã –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    content_no_media = re.sub(media_pattern, '', content, flags=re.DOTALL)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±—ã—á–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞
    rule_pattern = r'([^{}]+)\s*{([^{}]+)}'
    for match in re.finditer(rule_pattern, content_no_media):
        selectors = match.group(1).strip()
        properties = match.group(2).strip()
        
        if selectors and properties and not selectors.startswith('@'):
            rules.append({
                'selectors': selectors,
                'properties': properties,
                'full': match.group(0)
            })
    
    return rules, media_queries

def normalize_properties(props):
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å–≤–æ–π—Å—Ç–≤–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞
    prop_list = []
    for prop in props.split(';'):
        prop = prop.strip()
        if prop:
            # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
            prop = re.sub(r'\s+', ' ', prop)
            prop_list.append(prop)
    return sorted(prop_list)

def find_duplicate_rules(rules):
    """–ù–∞—Ö–æ–¥–∏—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –ø—Ä–∞–≤–∏–ª–∞"""
    duplicates = defaultdict(list)
    
    for i, rule in enumerate(rules):
        # –°–æ–∑–¥–∞—ë–º —Ö–µ—à –∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Å–≤–æ–π—Å—Ç–≤
        props_normalized = tuple(normalize_properties(rule['properties']))
        props_hash = hashlib.md5(str(props_normalized).encode()).hexdigest()
        
        duplicates[props_hash].append({
            'index': i,
            'selectors': rule['selectors'],
            'properties': rule['properties']
        })
    
    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –¥—É–±–ª–∏–∫–∞—Ç—ã (–≥–¥–µ –±–æ–ª—å—à–µ 1 –ø—Ä–∞–≤–∏–ª–∞)
    real_duplicates = {k: v for k, v in duplicates.items() if len(v) > 1}
    
    return real_duplicates

def find_similar_selectors(rules):
    """–ù–∞—Ö–æ–¥–∏—Ç —Å–µ–ª–µ–∫—Ç–æ—Ä—ã —Å –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏–º–∏ —Å–≤–æ–π—Å—Ç–≤–∞–º–∏"""
    similar = []
    
    for i in range(len(rules)):
        for j in range(i + 1, len(rules)):
            props1 = set(normalize_properties(rules[i]['properties']))
            props2 = set(normalize_properties(rules[j]['properties']))
            
            # –ï—Å–ª–∏ 80%+ —Å–≤–æ–π—Å—Ç–≤ —Å–æ–≤–ø–∞–¥–∞—é—Ç
            if len(props1) > 0 and len(props2) > 0:
                overlap = len(props1 & props2)
                similarity = overlap / max(len(props1), len(props2))
                
                if similarity >= 0.8:
                    similar.append({
                        'selector1': rules[i]['selectors'],
                        'selector2': rules[j]['selectors'],
                        'similarity': similarity,
                        'common_props': list(props1 & props2),
                        'unique1': list(props1 - props2),
                        'unique2': list(props2 - props1)
                    })
    
    return similar

def analyze_property_usage(rules):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–≤–æ–π—Å—Ç–≤"""
    property_count = defaultdict(int)
    property_values = defaultdict(set)
    
    for rule in rules:
        for prop in rule['properties'].split(';'):
            prop = prop.strip()
            if ':' in prop:
                name, value = prop.split(':', 1)
                name = name.strip()
                value = value.strip()
                property_count[name] += 1
                property_values[name].add(value)
    
    return property_count, property_values

def find_redundant_media_queries(media_queries):
    """–ù–∞—Ö–æ–¥–∏—Ç –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ –º–µ–¥–∏–∞-–∑–∞–ø—Ä–æ—Å—ã"""
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —É—Å–ª–æ–≤–∏—é
    grouped = defaultdict(list)
    
    for mq in media_queries:
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å
        query_normalized = re.sub(r'\s+', ' ', mq['query']).strip()
        grouped[query_normalized].append(mq)
    
    return grouped

def main():
    print("=" * 80)
    print("–ê–ì–†–ï–°–°–ò–í–ù–´–ô –ê–ù–ê–õ–ò–ó –î–£–ë–õ–ò–ö–ê–¢–û–í –ò –ò–ó–ë–´–¢–û–ß–ù–û–°–¢–ò")
    print("=" * 80)
    
    with open(CSS_FILE, 'r', encoding='utf-8') as f:
        content = f.read()
    
    print(f"\nüìä –ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    # –ü–∞—Ä—Å–∏–º CSS
    print("\nüîç –ü–∞—Ä—Å–∏–Ω–≥ CSS...")
    rules, media_queries = parse_css_rules(content)
    print(f"  ‚úì –ù–∞–π–¥–µ–Ω–æ –ø—Ä–∞–≤–∏–ª: {len(rules)}")
    print(f"  ‚úì –ù–∞–π–¥–µ–Ω–æ –º–µ–¥–∏–∞-–∑–∞–ø—Ä–æ—Å–æ–≤: {len(media_queries)}")
    
    # –ò—â–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    print("\nüîé –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")
    duplicates = find_duplicate_rules(rules)
    total_duplicate_rules = sum(len(v) - 1 for v in duplicates.values())
    print(f"  ‚úì –ù–∞–π–¥–µ–Ω–æ –≥—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(duplicates)}")
    print(f"  ‚úì –ú–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å –ø—Ä–∞–≤–∏–ª: {total_duplicate_rules}")
    
    # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
    print("\nüîé –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –ø—Ä–∞–≤–∏–ª (80%+ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)...")
    similar = find_similar_selectors(rules)
    print(f"  ‚úì –ù–∞–π–¥–µ–Ω–æ –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Ä: {len(similar)}")
    
    # –ê–Ω–∞–ª–∏–∑ —Å–≤–æ–π—Å—Ç–≤
    print("\nüìä –ê–Ω–∞–ª–∏–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–≤–æ–π—Å—Ç–≤...")
    prop_count, prop_values = analyze_property_usage(rules)
    print(f"  ‚úì –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–≤–æ–π—Å—Ç–≤: {len(prop_count)}")
    
    # –¢–æ–ø-10 —Å–∞–º—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Å–≤–æ–π—Å—Ç–≤
    top_props = sorted(prop_count.items(), key=lambda x: x[1], reverse=True)[:10]
    print("\n  üìå –¢–æ–ø-10 —Å–≤–æ–π—Å—Ç–≤:")
    for prop, count in top_props:
        print(f"     {prop}: {count} —Ä–∞–∑")
    
    # –ê–Ω–∞–ª–∏–∑ –º–µ–¥–∏–∞-–∑–∞–ø—Ä–æ—Å–æ–≤
    print("\nüîé –ê–Ω–∞–ª–∏–∑ –º–µ–¥–∏–∞-–∑–∞–ø—Ä–æ—Å–æ–≤...")
    grouped_mq = find_redundant_media_queries(media_queries)
    print(f"  ‚úì –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π: {len(grouped_mq)}")
    print(f"  ‚úì –ú–æ–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å: {len(media_queries) - len(grouped_mq)} –∑–∞–ø—Ä–æ—Å–æ–≤")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç
    with open('duplicate_analysis.txt', 'w', encoding='utf-8') as f:
        f.write("–ê–ù–ê–õ–ò–ó –î–£–ë–õ–ò–ö–ê–¢–û–í –ò –ò–ó–ë–´–¢–û–ß–ù–û–°–¢–ò\n")
        f.write("=" * 80 + "\n\n")
        
        f.write(f"–°–¢–ê–¢–ò–°–¢–ò–ö–ê:\n")
        f.write(f"  –í—Å–µ–≥–æ –ø—Ä–∞–≤–∏–ª: {len(rules)}\n")
        f.write(f"  –ì—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(duplicates)}\n")
        f.write(f"  –ú–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å: {total_duplicate_rules} –ø—Ä–∞–≤–∏–ª\n")
        f.write(f"  –ü–æ—Ö–æ–∂–∏—Ö –ø–∞—Ä: {len(similar)}\n")
        f.write(f"  –ú–µ–¥–∏–∞-–∑–∞–ø—Ä–æ—Å–æ–≤: {len(media_queries)}\n")
        f.write(f"  –ú–æ–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å: {len(media_queries) - len(grouped_mq)}\n\n")
        
        f.write("–î–£–ë–õ–ò–ö–ê–¢–´ (–ø–µ—Ä–≤—ã–µ 20 –≥—Ä—É–ø–ø):\n")
        f.write("-" * 80 + "\n")
        for i, (hash_val, dup_list) in enumerate(list(duplicates.items())[:20], 1):
            f.write(f"\n–ì—Ä—É–ø–ø–∞ {i} ({len(dup_list)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤):\n")
            for dup in dup_list:
                f.write(f"  –°–µ–ª–µ–∫—Ç–æ—Ä: {dup['selectors']}\n")
            f.write(f"  –°–≤–æ–π—Å—Ç–≤–∞: {dup_list[0]['properties'][:100]}...\n")
        
        f.write("\n\n–ü–û–•–û–ñ–ò–ï –ü–†–ê–í–ò–õ–ê (–ø–µ—Ä–≤—ã–µ 30):\n")
        f.write("-" * 80 + "\n")
        for i, sim in enumerate(similar[:30], 1):
            f.write(f"\n{i}. –ü–æ—Ö–æ–∂–µ—Å—Ç—å: {sim['similarity']:.0%}\n")
            f.write(f"   –°–µ–ª–µ–∫—Ç–æ—Ä 1: {sim['selector1']}\n")
            f.write(f"   –°–µ–ª–µ–∫—Ç–æ—Ä 2: {sim['selector2']}\n")
            f.write(f"   –û–±—â–∏—Ö —Å–≤–æ–π—Å—Ç–≤: {len(sim['common_props'])}\n")
            if sim['unique1']:
                f.write(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ 1: {', '.join(sim['unique1'][:3])}\n")
            if sim['unique2']:
                f.write(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ 2: {', '.join(sim['unique2'][:3])}\n")
        
        f.write("\n\n–ú–ï–î–ò–ê-–ó–ê–ü–†–û–°–´ –ü–û –ì–†–£–ü–ü–ê–ú:\n")
        f.write("-" * 80 + "\n")
        for query, mq_list in grouped_mq.items():
            f.write(f"\n@media {query} ({len(mq_list)} –≤—Ö–æ–∂–¥–µ–Ω–∏–π)\n")
    
    print(f"\nüìä –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç: duplicate_analysis.txt")
    
    # –ü–æ–¥—Å—á—ë—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π —ç–∫–æ–Ω–æ–º–∏–∏
    potential_savings = total_duplicate_rules
    print(f"\nüíæ –ü–û–¢–ï–ù–¶–ò–ê–õ–¨–ù–ê–Ø –≠–ö–û–ù–û–ú–ò–Ø:")
    print(f"   –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: ~{total_duplicate_rules} –ø—Ä–∞–≤–∏–ª")
    print(f"   –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö: ~{len(similar) // 2} –ø—Ä–∞–≤–∏–ª")
    print(f"   –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–µ–¥–∏–∞-–∑–∞–ø—Ä–æ—Å–æ–≤: ~{len(media_queries) - len(grouped_mq)} –±–ª–æ–∫–æ–≤")
    print(f"   –ò–¢–û–ì–û: –º–æ–∂–Ω–æ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –Ω–∞ ~{potential_savings + len(similar)//2} –ø—Ä–∞–≤–∏–ª")
    
    current_lines = len(content.split('\n'))
    estimated_lines = current_lines - (potential_savings + len(similar)//2) * 5
    print(f"\nüìè –û–¶–ï–ù–ö–ê –°–¢–†–û–ö:")
    print(f"   –°–µ–π—á–∞—Å: {current_lines} —Å—Ç—Ä–æ–∫")
    print(f"   –ü–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: ~{estimated_lines} —Å—Ç—Ä–æ–∫")
    print(f"   –≠–∫–æ–Ω–æ–º–∏—è: ~{current_lines - estimated_lines} —Å—Ç—Ä–æ–∫ ({(current_lines - estimated_lines) / current_lines * 100:.1f}%)")
    
    print("\n" + "=" * 80)

if __name__ == "__main__":
    main()
